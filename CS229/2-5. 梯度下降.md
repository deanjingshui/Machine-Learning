# Gradient descent

将函数代价最小化的方法



![](..\image\NG_梯度下降法.png)



:= 赋值运算符

$\alpha$ 学习速率 learning rate

注意：必须同时更新   $\theta_0$和 $\theta_1$



学习速率过小、过大：

这里简单起见，假设是单个参数 $\theta_1$  

![](..\image\NG_学习速率.png)

fail to converge 无法收敛

diverge  甚至发散



学习速度为定值，为何可以收敛：

因为导数的绝对值会逐渐变小，如下图

![](..\image\NG_梯度下降_学习速率_2.png)

